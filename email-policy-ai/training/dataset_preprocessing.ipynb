{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd379550",
   "metadata": {},
   "source": [
    "## üß† What is Ray?\n",
    "\n",
    "> **Ray** is a Python framework for **distributed computing**. It makes it easy to scale Python code across **multiple cores**, **GPUs**, **machines**, or **clusters**, **without rewriting your codebase**.\n",
    "\n",
    "---\n",
    "\n",
    "## üçΩ Real-World Analogy: Ray is Like a Restaurant Kitchen\n",
    "\n",
    "Imagine you‚Äôre a chef managing a busy restaurant kitchen:\n",
    "\n",
    "### Without Ray:\n",
    "\n",
    "You‚Äôre the **only cook**, making every dish yourself ‚Äî from slicing onions to baking pizzas. Even if you have 8 burners, you‚Äôre only using one.\n",
    "\n",
    "### With Ray:\n",
    "\n",
    "You become the **head chef** who assigns tasks to multiple **line cooks**:\n",
    "\n",
    "* One handles pasta\n",
    "* Another bakes pizzas\n",
    "* One prepares sauces\n",
    "* All **in parallel** ‚ö°\n",
    "\n",
    "You manage the workflow ‚Äî **they cook the food**.\n",
    "\n",
    "‚úÖ Result: Everything gets done **faster**, **cleaner**, and **at scale**\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äçüíª Programming Analogy: Regular Python vs. Ray\n",
    "\n",
    "### üîÅ Regular Python\n",
    "\n",
    "```python\n",
    "def slow_task(x):\n",
    "    time.sleep(1)\n",
    "    return x * 2\n",
    "\n",
    "results = [slow_task(x) for x in range(5)]\n",
    "```\n",
    "\n",
    "‚è±Ô∏è Takes \\~5 seconds (runs sequentially)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Ray Parallel Version\n",
    "\n",
    "```python\n",
    "import ray\n",
    "import time\n",
    "\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def slow_task(x):\n",
    "    time.sleep(1)\n",
    "    return x * 2\n",
    "\n",
    "futures = [slow_task.remote(x) for x in range(5)]\n",
    "results = ray.get(futures)\n",
    "```\n",
    "\n",
    "‚úÖ Takes \\~1 second (runs in parallel across CPU cores)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Ray Is Used in Enterprise Systems\n",
    "\n",
    "| Feature                  | Why It Matters                          |\n",
    "| ------------------------ | --------------------------------------- |\n",
    "| üîÑ Distributed Execution | Scale across CPUs, GPUs, clusters       |\n",
    "| üß† ML-native             | Supports HuggingFace, PyTorch, Sklearn  |\n",
    "| üîå Pluggable             | Use Ray with Airflow, Spark, Dask, etc. |\n",
    "| üìä Job Monitoring        | Has dashboards for profiling & tracing  |\n",
    "| üöÄ Easy to Start         | Works locally with 1 line: `ray.init()` |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Real-World Use in Your Project\n",
    "\n",
    "You're already using it like this:\n",
    "\n",
    "```python\n",
    "@ray.remote\n",
    "def preprocess_and_save():\n",
    "    ...\n",
    "```\n",
    "\n",
    "Here, you're:\n",
    "\n",
    "* Turning `preprocess_and_save()` into a **Ray task**\n",
    "* Running it as a **background distributed process**\n",
    "* Even though it's on your laptop now, this code can run on **10 nodes with zero change** later\n",
    "\n",
    "---\n",
    "\n",
    "## üî® When to Use Ray\n",
    "\n",
    "| Task Type                       | Ray Use? | Why                             |\n",
    "| ------------------------------- | -------- | ------------------------------- |\n",
    "| Preprocessing large datasets    | ‚úÖ Yes    | Parallel tokenization           |\n",
    "| Training models                 | ‚úÖ Yes    | Use `ray.train` or `RayTrainer` |\n",
    "| Running experiments/grid search | ‚úÖ Yes    | Ray Tune (for AutoML)           |\n",
    "| Small script with 100 rows      | ‚ùå No     | Too much overhead               |\n",
    "| RAG chatbot backend             | ‚ö†Ô∏è Maybe | Use if scaling is needed        |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Example: Training on Ray with HuggingFace\n",
    "\n",
    "```python\n",
    "from ray.train.huggingface import TransformersTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "trainer = TransformersTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(...),\n",
    "    scaling_config=ScalingConfig(num_workers=4),\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.fit()\n",
    "```\n",
    "\n",
    "‚úÖ Ray will:\n",
    "\n",
    "* Split your data\n",
    "* Distribute training across 4 workers\n",
    "* Aggregate results\n",
    "* Save the best model\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ TL;DR Summary\n",
    "\n",
    "| Concept          | Ray Explanation                                |\n",
    "| ---------------- | ---------------------------------------------- |\n",
    "| Remote function  | Run a function in parallel on a worker         |\n",
    "| Object store     | Ray tracks where data is, avoids copies        |\n",
    "| Cluster manager  | Run on 1 machine, a cloud cluster, or hybrid   |\n",
    "| Seamless scaling | Local dev ‚Üí Enterprise deployment (no rewrite) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5a9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "import ray  \n",
    "from ray import train as ray_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac9e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_PATH = \"/Users/tanmay/Downloads/US_Sol_LLM/email-policy-ai/data/raw/policy_labels.jsonl\"\n",
    "MODEL_NAME = \"distilroberta-base\"\n",
    "TEST_SIZE = 0.2\n",
    "MAX_LENGTH = 512\n",
    "SEED = 42\n",
    "OUTPUT_DIR = \"/Users/tanmay/Downloads/US_Sol_LLM/email-policy-ai/data/processed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95259515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7873f7ffe3542b992be5b1432789c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a459cc669e3c4ed1bbf91b59c3a20b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74cc1ffbb834aa09631b1aaf8a67d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0223a070365b4e62b8763c7b1dfa0c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cf7ef102984c03beff46266e1a195c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a890df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Normalize unicode characters (e.g., curly quotes to ASCII)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    return text.encode(\"ascii\", \"ignore\").decode(\"utf-8\").strip()\n",
    "\n",
    "def load_jsonl(path):\n",
    "    records = []\n",
    "    labels_set = set()\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                text = normalize_text(obj.get(\"text\", \"\"))\n",
    "                label = obj.get(\"label\", \"\").strip()\n",
    "                if text and label:\n",
    "                    labels_set.add(label)\n",
    "                    records.append({\"text\": text, \"label\": label})\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ùå JSON parse error on line {i}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Create label2id mappings\n",
    "    label2id = {label: idx for idx, label in enumerate(sorted(labels_set))}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    df[\"label\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "    return df, label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636493f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß† What Does This Line Do?\n",
    "\n",
    "```python\n",
    "tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"np\")\n",
    "```\n",
    "\n",
    "This takes raw email text and returns a **machine-readable representation** for the transformer model. Let‚Äôs dive deeper:\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Step-by-Step Breakdown\n",
    "\n",
    "### 1Ô∏è‚É£ **Tokenization**\n",
    "\n",
    "* The tokenizer breaks the raw string into **subword tokens**.\n",
    "\n",
    "  * For example:\n",
    "\n",
    "    ```\n",
    "    \"Please email me your credit card number.\"\n",
    "    ‚ü∂ ['Please', 'ƒ†email', 'ƒ†me', 'ƒ†your', 'ƒ†credit', 'ƒ†card', 'ƒ†number', '.']\n",
    "    ```\n",
    "\n",
    "### 2Ô∏è‚É£ **Input IDs**\n",
    "\n",
    "* Each token is mapped to an integer from the model‚Äôs vocabulary.\n",
    "\n",
    "  ```python\n",
    "  [1423, 3412, 115, 2468, 5123, 872, 9321, 4]\n",
    "  ```\n",
    "\n",
    "### 3Ô∏è‚É£ **Attention Mask**\n",
    "\n",
    "* A binary mask:\n",
    "\n",
    "  * `1` = this token should be attended to (actual content)\n",
    "  * `0` = this token is just padding\n",
    "  * Used during model‚Äôs attention calculations\n",
    "\n",
    "### 4Ô∏è‚É£ **Truncation**\n",
    "\n",
    "* If the email text becomes **more than 512 tokens**, only the first 512 tokens are kept.\n",
    "* This prevents model failure due to exceeding its limit.\n",
    "\n",
    "### 5Ô∏è‚É£ **Padding**\n",
    "\n",
    "* If the email is **shorter than 512 tokens**, it's padded with special `[PAD]` tokens (usually ID `0`).\n",
    "* Ensures every input is **the same length** ‚Äî required for efficient batching on GPUs.\n",
    "\n",
    "### 6Ô∏è‚É£ **return\\_tensors=\"np\"**\n",
    "\n",
    "* Returns the data as a NumPy array ‚Äî needed for Ray/HuggingFace batch processing during preprocessing.\n",
    "* You later convert to torch tensors for training.\n",
    "\n",
    "---\n",
    "\n",
    "## üè≠ Why This Matters in Enterprise NLP Systems\n",
    "\n",
    "| Feature           | Why It‚Äôs Enterprise-Standard                                  |\n",
    "| ----------------- | ------------------------------------------------------------- |\n",
    "| ‚öñÔ∏è Uniform length | Makes batched training possible (no dynamic shape headaches)  |\n",
    "| üß† Attention mask | Allows the model to **ignore padding** tokens during training |\n",
    "| ‚ö° Max efficiency  | Enables GPU-friendly input formats (512 tokens = \\~128 words) |\n",
    "| üßπ Clean design   | Avoids runtime errors from overly long emails                 |\n",
    "| üì• Email-safe     | Works even on long enterprise emails, disclaimers, threads    |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Final Output (Example)\n",
    "\n",
    "For one email:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input_ids\": [101, 1423, 3412, 115, 2468, ..., 0, 0, 0],\n",
    "  \"attention_mask\": [1, 1, 1, 1, 1, ..., 0, 0, 0],\n",
    "  \"label\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "* `input_ids` ‚Üí tokenized and padded text\n",
    "* `attention_mask` ‚Üí 1s for real text, 0s for padding\n",
    "* `label` ‚Üí 1 (violation) or 0 (compliant)\n",
    "\n",
    "This is now ready for `torch.utils.data.DataLoader` or direct `Trainer` API usage.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ TL;DR\n",
    "\n",
    "| Part                 | Purpose                              |\n",
    "| -------------------- | ------------------------------------ |\n",
    "| `input_ids`          | What the model sees                  |\n",
    "| `attention_mask`     | What the model should **ignore**     |\n",
    "| `padding/truncation` | Ensures every email is 512 tokens    |\n",
    "| NumPy/Torch format   | Enables efficient training, batching |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05de0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"],padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba7a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    df=load_jsonl(LABEL_PATH)\n",
    "    train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=SEED, stratify=df['label'])\n",
    "    dataset=DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "        \"test\": Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "    })\n",
    "    tokenized_dataset= dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d4916c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def preprocess_and_save():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    dataset=prepare_dataset()\n",
    "    dataset[\"train\"].to_json(os.path.join(OUTPUT_DIR, \"train_tokenized.json\"))\n",
    "    dataset[\"test\"].to_json(os.path.join(OUTPUT_DIR, \"test_tokenized.json\"))\n",
    "    print(\"Preprocessing and saving completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d872b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if __name__ == \"__main__\":\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    ray.get(preprocess_and_save.remote())\n",
    "    ray.shutdown()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
