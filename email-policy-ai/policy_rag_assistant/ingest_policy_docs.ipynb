{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f15547c",
   "metadata": {},
   "source": [
    "\n",
    "### 🧠 What is Qdrant?\n",
    "\n",
    "Qdrant (pronounced “quadrant”) is a **vector database** — a special kind of database designed to **store and search text by meaning** instead of exact words.\n",
    "\n",
    "---\n",
    "\n",
    "### 🍕 Real-World Analogy: Qdrant is Like a Smart Pizza Finder\n",
    "\n",
    "#### Imagine this:\n",
    "\n",
    "You own a massive **pizza recipe book** with **10,000+ pages**. Each page has one pizza recipe.\n",
    "\n",
    "Now you want to find:\n",
    "\n",
    "> “A pizza that uses spicy meat, not too cheesy, and has thin crust.”\n",
    "\n",
    "### 🔍 What a regular database would do:\n",
    "\n",
    "It looks for **exact words** like “spicy”, “cheesy”, “crust” and gives you any recipe that contains those words — even if it’s irrelevant.\n",
    "\n",
    "### 🧠 What Qdrant does:\n",
    "\n",
    "* It **converts each recipe** into a **mathematical vector** that captures the **meaning** of the recipe (spicy, texture, toppings, etc.).\n",
    "* It also converts your search query into a vector.\n",
    "* Then it **compares meanings**, not just words, and shows you the **closest match**.\n",
    "\n",
    "👉 You say “spicy meat, thin crust”, and it gives you:\n",
    "🧀 “Calabrese Pepperoni Pizza” — even if the words don’t match exactly.\n",
    "\n",
    "---\n",
    "\n",
    "### 💼 In Your Case:\n",
    "\n",
    "* **PDF policies** are like your recipe book.\n",
    "* **Each policy rule** is like a page or paragraph.\n",
    "* **Qdrant helps your AI system quickly find the most relevant policy rule** when someone types:\n",
    "\n",
    "  > “Can I email sensitive info over Gmail?”\n",
    "\n",
    "Even if “Gmail” or “email” isn’t mentioned exactly, Qdrant will find similar concepts like:\n",
    "\n",
    "> “Public email services must not be used for confidential data.”\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why Use Qdrant (vs traditional DBs)?\n",
    "\n",
    "| Feature           | Qdrant (Vector DB)                 | SQL/NoSQL (Traditional)        |\n",
    "| ----------------- | ---------------------------------- | ------------------------------ |\n",
    "| Search by meaning | ✅ Yes (semantic search)            | ❌ Only keyword match           |\n",
    "| Scalable for AI   | ✅ Yes, handles 100K+ vectors fast  | ❌ Not optimized for embeddings |\n",
    "| Built for LLMs    | ✅ Integrates with embedding models | ❌ Not designed for this        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cf829",
   "metadata": {},
   "source": [
    "\n",
    "## 🧠 TL;DR:\n",
    "\n",
    "| Feature        | **Qdrant**                          | **ChromaDB**                         |\n",
    "| -------------- | ----------------------------------- | ------------------------------------ |\n",
    "| Best for       | Scalable, production AI search      | Fast local prototyping with LLMs     |\n",
    "| Think of it as | A smart **library server**          | A lightweight **personal notebook**  |\n",
    "| Deploys on     | Local or cloud, works well at scale | Local file system (no network infra) |\n",
    "| Built for      | Enterprise-grade search + scale     | Fast, easy local dev & prototyping   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🍕 Analogy: A Pizza Restaurant's Recipe Management System\n",
    "\n",
    "### 📚 Qdrant = **Enterprise Recipe Library System**\n",
    "\n",
    "Imagine you're a large pizza chain with 200 locations.\n",
    "\n",
    "* You store **all your recipes in a central database**.\n",
    "* You want any chef in any location to search:\n",
    "\n",
    "  > “Show me low-fat spicy meat pizzas”\n",
    "* The system:\n",
    "\n",
    "  * Understands **meaning**\n",
    "  * Returns **relevant recipes**\n",
    "  * Works fast even with **millions of recipes**\n",
    "  * Can be deployed in **cloud or production server**\n",
    "\n",
    "🔧 That’s **Qdrant**:\n",
    "✅ Fast\n",
    "✅ Smart (semantic search)\n",
    "✅ Scalable\n",
    "✅ Reliable in enterprise environments\n",
    "\n",
    "---\n",
    "\n",
    "### 🗒️ ChromaDB = **Chef’s Personal Recipe Notebook**\n",
    "\n",
    "Now imagine you’re a single pizza chef experimenting at home.\n",
    "\n",
    "* You keep **10-50 recipes** on your laptop.\n",
    "* You want to quickly search:\n",
    "\n",
    "  > “spicy crustless vegan pizza”\n",
    "* You don't care about multi-location support, just quick local lookup\n",
    "* You just use a **simple notebook with some tags**.\n",
    "\n",
    "🟢 That’s **ChromaDB**:\n",
    "✅ Super fast locally\n",
    "✅ No setup\n",
    "✅ Perfect for personal use or small experiments\n",
    "❌ Not made for production or large scale\n",
    "\n",
    "---\n",
    "\n",
    "## 💼 When to Use What?\n",
    "\n",
    "| Use Case                                  | Qdrant        | ChromaDB     |\n",
    "| ----------------------------------------- | ------------- | ------------ |\n",
    "| Local LLM prototype (1 dev)               | ❌ Overkill    | ✅ Ideal      |\n",
    "| Production compliance engine (like yours) | ✅ Perfect     | ❌ Not enough |\n",
    "| Works with Azure, Docker, cloud infra     | ✅ Yes         | ⚠️ Hacky     |\n",
    "| Needs full-text + vector search           | ✅ Has filters | ❌ Basic only |\n",
    "| Persistent, shared across services        | ✅ Yes         | ❌ Not yet    |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Recommendation for You\n",
    "\n",
    "Since you're building:\n",
    "\n",
    "* A **compliance system**\n",
    "* For **enterprise**\n",
    "* That uses **20+ policies and growing**\n",
    "* With **semantic lookups and real users**\n",
    "\n",
    "➡️ **Stick with Qdrant**. It’s production-ready, fast, and scalable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f7452d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz                # To read text from PDFs (PyMuPDF)\n",
    "import os, re, json        # File handling and regular expressions\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np        # For numerical operations\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f102e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = \"../data/policy_docs\"\n",
    "PROCESSED_JSON = \"../data/processed/policy_rules.json\"\n",
    "QDRANT_HOST = \"localhost\"\n",
    "QDRANT_PORT = 6333\n",
    "COLLECTION_NAME = \"policy_rules\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4886ee9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 What is a Distilled Embedding Model?\n",
    "\n",
    "> A **distilled model** is a smaller, faster, and more efficient version of a large language model (LLM), trained to mimic its performance but with fewer parameters.\n",
    "\n",
    "💡 Think of it as:\n",
    "\n",
    "* The **student** who learns from a larger **teacher model** (e.g., BERT, RoBERTa)\n",
    "* Much **faster** to run\n",
    "* Slightly less accurate, but **good enough** for real-time systems like email compliance, chatbots, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏭 In Enterprise, Why Do You Use Distilled Models?\n",
    "\n",
    "| Concern                 | Distilled Model Solves It                                  |\n",
    "| ----------------------- | ---------------------------------------------------------- |\n",
    "| 🔥 Real-time latency    | Fast inference (<100ms on CPU/GPU)                         |\n",
    "| 🧠 Memory usage         | Smaller size (100MB–500MB vs. multi-GB)                    |\n",
    "| 🧾 Deployability        | Easier to run in Docker, AKS, edge devices, or local boxes |\n",
    "| ✅ Cost control          | No GPU or expensive cloud models needed                    |\n",
    "| 📦 Model auditability   | Static, deterministic, and explainable                     |\n",
    "| 🚫 No cloud fine-tuning | You can fine-tune it **locally or on Ray**                 |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Recommended Distilled Embedding Models (Enterprise Perspective)\n",
    "\n",
    "Here’s how to choose, based on your use case (email + policy matching):\n",
    "\n",
    "| Model Name                     | Type         | Size    | Dim | Pros                                           | When to Use                       |\n",
    "| ------------------------------ | ------------ | ------- | --- | ---------------------------------------------- | --------------------------------- |\n",
    "| `all-MiniLM-L6-v2`             | SentenceBERT | \\~80MB  | 384 | Fast, high-quality sentence-level embeddings   | ✅ Default go-to model             |\n",
    "| `distilroberta-base`           | Encoder      | \\~300MB | 768 | Good for fine-tuning, classification           | Use if building a classifier      |\n",
    "| `TinyLlama-1.1B`               | LLM          | \\~500MB | N/A | Smallest LLM-style, for Q\\&A or classification | For chat-based tools              |\n",
    "| `intfloat/e5-small`            | Embeddings   | \\~90MB  | 384 | Optimized for semantic search + RAG            | For high-recall retrieval         |\n",
    "| `nomic-ai/nomic-embed-text-v1` | Embedder     | \\~120MB | 768 | Multi-purpose, newer                           | For newer vector DBs like LanceDB |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 For You — Recommended Model\n",
    "\n",
    "Since you’re doing **policy violation detection + RAG Q\\&A**, your use case involves both:\n",
    "\n",
    "1. **Email/Document Embedding for Search**\n",
    "2. **Policy Chunk Embedding for Retrieval**\n",
    "\n",
    "✅ **Best Option:** `all-MiniLM-L6-v2`\n",
    "\n",
    "* Balanced: Small, fast, enterprise-tested\n",
    "* 384-dim vector (works well with Qdrant)\n",
    "* Already supported by `sentence-transformers`\n",
    "* Good zero-shot sentence similarity\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ When to Consider Alternatives\n",
    "\n",
    "| Situation                                | Consider Instead              |\n",
    "| ---------------------------------------- | ----------------------------- |\n",
    "| Need more accuracy, can afford 2× slower | `all-MiniLM-L12-v2`           |\n",
    "| Want better performance on short phrases | `e5-small` or `mpnet-base-v2` |\n",
    "| Need general LLM for chat + generation   | `TinyLlama` + LoRA tuning     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Enterprise Notes\n",
    "\n",
    "* Always pin versions in `requirements.txt` for reproducibility\n",
    "* Benchmark on a **small validation set** (your labeled JSONL)\n",
    "* Track `embedding_dim`, `model_name`, and `vector_index_id` in logs/metadata\n",
    "* Store model artifacts in a versioned location (Azure Blob, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔚 Summary\n",
    "\n",
    "| What You Need                    | Recommended Distilled Model |\n",
    "| -------------------------------- | --------------------------- |\n",
    "| Email + Policy semantic matching | ✅ `all-MiniLM-L6-v2`        |\n",
    "| Lightweight, fast, deployable    | ✅ Yes                       |\n",
    "| Good for RAG + compliance chat   | ✅ Yes                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4777736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb14acf67ac1483892d708bf3436e4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3a395884df4ea798638aefcba21a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c2131b59c54be09f1471802b59035d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7f2921588d42f898c788ea2edaeb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd14c91b280400a920041a969f1cf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c486c4070ecd4e1fb5b5f834ae97d1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5ef04dd0d149cca58d6fb4e750422e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b415be0da975423b9e36fbef262d81e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089ed293ef71403aa54fa1c6a0781281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c9eb3a74f440b18111505c7abbafd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26bb085f8ef47d4acf07bf7bb03c446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2416a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ✅ What It's Doing\n",
    "\n",
    "1. **`SentenceTransformer(...)`**:\n",
    "\n",
    "   * Loads a **pretrained embedding model** (like `all-MiniLM-L6-v2`)\n",
    "   * This model **converts a sentence into a fixed-size vector**\n",
    "   * These vectors represent the **meaning** of the text\n",
    "\n",
    "2. **`QdrantClient(...)`**:\n",
    "\n",
    "   * Connects to your **local or remote Qdrant vector database**\n",
    "   * Allows you to **insert, search, or manage** semantic data\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Technical Explanation\n",
    "\n",
    "### 1️⃣ `SentenceTransformer(EMBEDDING_MODEL)`\n",
    "\n",
    "* Loads a distilled transformer model (MiniLM) trained using **Siamese / triplet learning** on sentence pairs\n",
    "* When you call:\n",
    "\n",
    "  ```python\n",
    "  model.encode(\"Can I send client data via Gmail?\")\n",
    "  ```\n",
    "\n",
    "  It returns a **384-dimensional vector** like:\n",
    "\n",
    "  ```python\n",
    "  [0.11, -0.42, ..., 0.03]  # embedding\n",
    "  ```\n",
    "\n",
    "💡 These embeddings are:\n",
    "\n",
    "* **Dense** (not sparse like TF-IDF)\n",
    "* **Context-aware**\n",
    "* **Universal** across topics (policies, emails, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ `QdrantClient(...)`\n",
    "\n",
    "* Connects to your Qdrant instance running on `localhost:6333`\n",
    "* Lets you **push** and **query** vectors along with **metadata** (like title, file, date)\n",
    "* Used later for **semantic search** or **chatbot retrieval**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Real-World Analogy\n",
    "\n",
    "### Imagine this is a Library System\n",
    "\n",
    "| Component                  | Real World Analogy                                                                                                                               |\n",
    "| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `SentenceTransformer(...)` | A **smart librarian** who reads each document and summarizes its meaning as a special “DNA code” (vector)                                        |\n",
    "| `QdrantClient(...)`        | A **digital filing cabinet** that stores those DNA codes, indexed for super-fast lookup                                                          |\n",
    "| Search later               | Asking the librarian: “Which policies talk about remote access over Gmail?” and she pulls the 3 most semantically related pages from the cabinet |\n",
    "\n",
    "---\n",
    "\n",
    "## 🏭 Why It’s Used in Industry\n",
    "\n",
    "| Feature               | Why It Matters in Enterprise                          |\n",
    "| --------------------- | ----------------------------------------------------- |\n",
    "| `SentenceTransformer` | Fast, accurate semantic embedding without fine-tuning |\n",
    "| Embeddings            | Enables similarity search, clustering, classification |\n",
    "| `QdrantClient`        | Easy integration with scalable vector DB              |\n",
    "| Fast + Explainable    | Runs fast, gives traceable results (with metadata)    |\n",
    "\n",
    "Used in:\n",
    "\n",
    "* **Chatbots**\n",
    "* **Compliance scanners**\n",
    "* **Semantic search portals**\n",
    "* **Email/document matching engines**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34a3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    sections = []\n",
    "    pattern = re.compile(r\"(?<=\\n)(\\d{1,2}\\.?\\s?[A-Z].{5,}?)(?=\\n)\")\n",
    "    matches = list(pattern.finditer(text))\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        heading = match.group(1).strip()\n",
    "        content = text[start:end].strip()\n",
    "        if content:\n",
    "            sections.append({\"title\": heading, \"content\": content})\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42629c3b",
   "metadata": {},
   "source": [
    "Absolutely! This function `extract_sections(text)` is designed to **split a block of policy document text into structured sections** like \"1. Introduction\", \"2. Purpose\", etc.\n",
    "\n",
    "Let me break it down line-by-line and explain the logic behind it — including a **real-world analogy**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Purpose of the Function\n",
    "\n",
    "To extract **titled sections** from a plain-text version of a PDF, such as:\n",
    "\n",
    "```\n",
    "1. Introduction\n",
    "This policy outlines the purpose of...\n",
    "\n",
    "2. Purpose\n",
    "To ensure secure handling of...\n",
    "\n",
    "3. Scope\n",
    "This applies to all employees...\n",
    "```\n",
    "\n",
    "➡️ The goal is to return:\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\"title\": \"1. Introduction\", \"content\": \"This policy outlines...\"},\n",
    "  {\"title\": \"2. Purpose\", \"content\": \"To ensure secure handling...\"},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Line-by-Line Breakdown\n",
    "\n",
    "```python\n",
    "def extract_sections(text):\n",
    "    sections = []\n",
    "```\n",
    "\n",
    "✅ Starts with an empty list to store extracted sections.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    pattern = re.compile(r\"(?<=\\n)(\\d{1,2}\\.?\\s?[A-Z][^]{5,}?)(?=\\n)\")\n",
    "```\n",
    "\n",
    "🧠 This regex looks for lines like:\n",
    "\n",
    "* `1. Introduction`\n",
    "* `2 Purpose`\n",
    "* `11. Legal Compliance`\n",
    "\n",
    "### ⚙️ Regex Explained:\n",
    "\n",
    "| Part         | Meaning                                           |\n",
    "| ------------ | ------------------------------------------------- |\n",
    "| `(?<=\\n)`    | Look only if the line starts after a newline      |\n",
    "| `\\d{1,2}\\.?` | Match section numbers like `1` or `12.`           |\n",
    "| `\\s?`        | Optional space                                    |\n",
    "| `[A-Z]`      | First letter of the title must be capitalized     |\n",
    "| `[^]{5,}?`   | Rest of the title should be at least 5 characters |\n",
    "| `(?=\\n)`     | Ends at the next newline                          |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    matches = list(pattern.finditer(text))\n",
    "```\n",
    "\n",
    "🔍 Finds all matches of section headings in the document.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "```\n",
    "\n",
    "📏 For each section title:\n",
    "\n",
    "* `start` is where the title ends\n",
    "* `end` is where the **next** section starts\n",
    "* So you're grabbing the **text between two titles** — the section body\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        heading = match.group(1).strip()\n",
    "        content = text[start:end].strip()\n",
    "```\n",
    "\n",
    "📝 Clean the section title and its content.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        if content:\n",
    "            sections.append({\"title\": heading, \"content\": content})\n",
    "```\n",
    "\n",
    "✅ Only store non-empty sections to avoid noise.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    return sections\n",
    "```\n",
    "\n",
    "📤 Returns a list of structured sections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8170fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(file_path):\n",
    "    return \"\\n\".join([page.get_text() for page in fitz.open(file_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abfa86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs(pdf_dir):\n",
    "    all_sections = []\n",
    "    for file in os.listdir(pdf_dir):\n",
    "        if not file.endswith(\".pdf\"):\n",
    "            continue\n",
    "        text = pdf_to_text(os.path.join(pdf_dir, file))\n",
    "        sections = extract_sections(text)\n",
    "\n",
    "        meta = {\n",
    "            \"source\": file,\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        }\n",
    "\n",
    "        for section in sections:\n",
    "            record = {\n",
    "                \"title\": section[\"title\"],\n",
    "                \"content\": section[\"content\"].lower(),\n",
    "                \"metadata\": meta,\n",
    "            }\n",
    "            all_sections.append(record)\n",
    "    return all_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d144d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa043408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_qdrant(records):\n",
    "    if COLLECTION_NAME not in client.get_collections().collections:\n",
    "        client.recreate_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=-model.get_sentence_embedding_dimension(), distance=Distance.COSINE))\n",
    "        points = []\n",
    "        for idx, rec in enumerate(records):\n",
    "            vector=model.encode(rec[\"content\"]).tolist()\n",
    "            payload={\n",
    "                \"title\": rec[\"title\"],\n",
    "                \"content\": rec[\"content\"],\n",
    "                \"metadata\": rec[\"metadata\"]\n",
    "            }\n",
    "            points.append(PointStruct(id=idx, vector=vector, payload=payload))\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=points\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if __name__ == \"__main__\":\n",
    "    records = process_pdfs(PDF_DIR)\n",
    "    save_json(records, PROCESSED_JSON)\n",
    "    upload_to_qdrant(records)\n",
    "    print(f\"✅ Processed and indexed {len(records)} policy sections.\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
